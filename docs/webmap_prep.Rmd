---
title: "Preparing Reference Layers for a Web Map"
author: "<a href='https://ucanr.edu/?facultyid=32909' target='_blank'>Andy Lyons</a>"
date: "August 2022"
output:
  html_document: 
    mathjax: null
    self_contained: no
    lib_dir: lib
    includes:
      in_header: gtag_technotes.js
      before_body: tech_note_header.html
      after_body: tech_note_footer.html
---



```{css echo = FALSE}
h1.title {font-weight:bold;}
h1 {font-size:30px; font-weight:bold;}
h2 {font-size:24px; font-weight:bold;}
h3 {font-size:18px; font-weight:bold;}
h4.date {font-size:90%;}
```

<p style="font-style:italic; font-size:90%; margin:2em;">IGIS <a href="http://igis.ucanr.edu/Tech_Notes/">Tech Notes</a> describe workflows and techniques or using geospatial science and technologies in research and extension. They are works in progress, and we welcome feedback and comments.</p>

## Background

Web maps commonly include reference layers, such as property boundaries, administrative areas, infrastructure features, etc. Such reference layers are often maintained by an organization such as a government agency, research unit, land owner, company, etc. 

Reference layers are typically not hard to find, because the organizations that maintain generally make them publicly available. That being said, reference layers often require some manipulations before you can use them in a web map, and like everything else will need updates from time-to-time. 

If you're lucky (or your goals are flexible), you may be able to use a reference layer 'as-is'. More likely, you may have to do one or more of the following manipulations to get a reference layer 'ready' for a web map, depending on how you want the web map to function and what you get from the reference layer. 

- dissolve simple polygons into multipart polygons based on an attribute 
- explode multipart polygons into simple polygons  
- generate text fields for labels and popup windows  
- omit features  
- split subsets of features into different layers  
- remove unneeded fields in the attribute table
- compute the area and/or perimeter of featuers in specific units (e.g, acres)
- standardize field names
- change the capitalization of values in the attribute table
- repair invalid geometries  
- get rid of z-values
- project to Web Mercator
- create a point version of a polygon layer to be visible when zoomed out

\

The first step is to determine what needs to be done to get the 'authoritative' reference data ready for your web map. This will largely depend on the design and functionality of your web map (see table below for some examples). Once you figure out what you need to do, all of the above can be done using desktop GIS software like ArcGIS Pro or QGIS. Some operations can also be done in your web mapping platforms (like ArcGIS.com).

<div style="float:right; padding:10px; text-align:center;">
![](./images/worried_bg-trans_80x80.png){style="padding:2px;"}<br/>
*I have to do<br/>that again?*
</div>

\

The **pain point** however comes when a new version of the authoritative data is released by the publisher. The changes may only be to a small number of features, but if you want your web map to be current you now have to repeat the entire process of transforming the data for your web map. Hopefully you wrote down that sequence of geoprocessing steps you carefully developed 15 months ago? Hopefully it won't take too long to go through the workflow all over again?

<div style="clear:both;">\
</div>

## Scripting to the Rescue!

This is a good example where scripting can pay off - handsomely. If you do your data prep using a scripting language like R or Python, repeating the processing becomes a whole lot easier. And not only is it easier, you're also less likely to forget a step. Your web map will thank you!

Scripting is not a panacea for everything, because it almost always takes more time to write code than use a "point and click" tool. But transforming reference layers that you don't control for a web map that you want to stay current is a use case where scripting makes a whole lot of sense.  

The table below shows some of the common operations needed in web mapping, and the corresponding R functions you can use from the `sf` and `dplyr` packages. Further below, we'll see an actual example.

\

```{r functions_table, echo=FALSE, results='asis'}
library(kableExtra)

tbl_vec <- c("Operation",
             "Use Case",
             "R function(s)",
             
             "Dissolve simple polygons into multipart polygons", 
             "Sometimes multiple simple polygons belong to the same entity (e.g., a park may consist of several pieces of land). In your web map, you may want labels and popups to be associated at the entity level, hence you need to dissolve boundaries based on attribute values.",
             "`dplyr::group_by()` followed by `dplyr::summarise()`",
             
             "Explode multipart polygons into simple polygons", 
             "In some web maps, you might not want multipart polygons. For example, the individual parts of a parent feature may need to be treated independently for intersections or management. Think for example of a county which includes distant islands that are technically part of the county but should be treated as separate areas on your web map.", 
             "`st_cast()`",
             
             "Delete unneeded fields in the attribute table", 
             "Most reference datasets have extra fields in the attribute table that you really don't need in your web map (sometimes there are *lots* of extra fields!). Getting rid of them reduces overhead, improves performance, and simplifies building the web map.", 
             "`dplyr::select()`",
             
             "Generate text fields for labels and popup windows", 
             "Most web platforms give you some ability to customize text for labels and popup windows. This includes basic character formatting,  concatenating fields, adding prefixes and punctuation, wrapping text in HTML tags, etc. More complex text manipulation, such as search and replace functions, word swaps, or grabbing text from lookup tables is a lot easier to do in a script.", 
             "`dplyr::mutate()` with `paste()` and various functions from the `stringr` package",
             
             "Filter features", 
             "There are many cases where you only need some of the features in a reference layer for your web map. If your preferred features can be idenified via attribute fields, and/or their spatial relationship to your area of interest, it is easy to filter them with your script.", 
             "`dplyr::filter()`, `st_intersects()`, `st_disjoint()`, `st_contains()`, etc.",
             
             "Split subsets of features into different layers", 
             "Reference datasets may contain many types or categories of features, that you might want separated in your web map. For example a 'Places' layer may contain features for libraries, court houses, schools, fire stations, etc. Splitting these up into separate datasets can make your web map easier to navigate and reduce overhead.", 
             "`dplyr::filter()`",
             
             "Compute area and/or perimeter in specific units",
             "It is not uncommon to get a polygon dataset with a field called 'Shape_Area' containing the area in map units. ArcGIS Online will even create this field for you when you upload spatial data. However map units squared are often ginormous numbers not useful on web maps. Computing area in acres, hectares, miles squared, etc. is easy to build into your data processing pieline, and you can even specify the number of decimal places to keep.",
             "`sf::st_area()`, `units::set_units()`, `round()`",

             "Standardize field names",
             "You may find it easier to build your web map if the field names follow a standard naming conventions, particularly if you create custom expressions using Arcade or Javascript. This may involve converting everything to lower case, replacing spaces with underscores, etc.", 
             "`stringr::str_to_upper()`, `stringr::str_to_lower()`, `stringr::str_replace()`",
             
             "Change capitalization",
             "Sometimes you get GIS layers that use ALL CAPS for things like place names, labels, and even comments. You may not want to keep ALL CAPS in your web map, which offer other formatting options to indicate officialness or emphasis.",
             "`stringr::str_to_upper()`, `stringr::str_to_lower()`, `stringr::str_to_title()`",
             
             "Repair invalid geometries", 
             "It is not uncommon to get invalid geometries in vector data, which could include weird overlapping segments of a polygon, slivers smaller than the tolerance level, topology errors, etc. Contact the data producer to let them know, but in the meantime many (but not all) geometries errors can fixed in R.", 
             "`sf::st_make_valid()`",
             
             "Get rid of z-values",
             "It's not uncommon to get a vector layer with z-values that are all 0. Unless the features are on the surface of the ocean, this usually an error in the production. Even if the z-values are valid, they're usually not useful for web mapping unless you're doing 3D rendering. Fortunately they're easily removed.", 
             "`sf::st_zm()`",
             
             "Project to Web Mercator", 
             "Web Mercator is the coordinate reference system for almost all web map platforms (including ERSI). Although your web mapping engine may reproject data to web mercator on the fly, it's easy enough to build into a pipeline. Plus you'll be able to control and check the results locally.", 
             "`st_transform(3857)`",
             
             "Create a point version of a polygon layer",
             "Small polgyons are often not visible on a web map when you're zoomed out. A common workaround is to add an additional point version of the layer (because point layers draw at every scale). The point layer is configured to dislay at small scales (zoomed out), and as the user zooms in the polygon layer appears.", 
             "`sf::st_centroid()`")

ncol <- 3
tbl_mat <- matrix(tbl_vec[-(1:ncol)], byrow=TRUE, ncol=ncol, dimnames=list(1:((length(tbl_vec)-ncol)/ncol), tbl_vec[1:ncol]))

knitr::kable(tbl_mat, format = "html") %>%   #, table.attr='class="tbl_compact"') 
  kable_styling(bootstrap_options = c("striped", "condensed"),  full_width = TRUE) %>% 
  column_spec(column=1, width = "16em") 
```

\

## Uploading the Cleaned Up Reference Layer to Your Web Map

After you've got the reference layer ready to go, the next step is to upload it to your web mapping platform. This generally involves saving the layer to disk, and then uploading it via the browser.

The best file format for saving the data will vary depending on the web mapping platform. ArcGIS.com supports a number of [spatial file formats](https://doc.arcgis.com/en/arcgis-online/reference/supported-items.htm) you can use to upload your data. I would recommend GeoJSON in most cases. GeoJSON is easily exported from R, and ArcGIS.com can create a hosted feature layer without any additional steps or settings. 
A couple of caveats about GeoJSON files include:

- A single GeoJSON file can only contain one layer (more specifically, the gdal driver that sf and other packages use to write GeoJSON files to disk only supports a single layer per .geojson file).

- GeoJSON is a plain text format, This makes it easy to read with any text editor, however it can also result in large files. If your dataset is particularly big, consider thinning it down further or use one of the alternate formats below. 

- GeoJSON can handle projections. Projecting your layer to  Web Mercator (epsg 3857) before you upload it is strongly recommended so this operation doesn't have to be done on the server.

Other file formats that ArcGIS.com can use as the data source for a hosted feature layer include Shapefiles (zipped) and file geodatabases (zipped). 

\

# Example: Preparing  REC boundaries for a Web Map

The rest of this Tech Note will work through an example of prepping an authoritative reference layer for a web map using R. In this case, we'll be making some tweaks to the authoritative boundary layer for [Research and Extension Centers](https://recs.ucanr.edu/){target="_blank" rel="noopener"} (RECs) under the University of California Division of Agriculture and Natural Resources ([UCANR](https://ucanr.edu/){target="_blank" rel="noopener"}).

\

### Import the Authoritative Layer

Step 1 is to import the authoritative reference layer into R. In some cases, this may require downloading a dataset to your hard drive as a Shapefile or file geodatabase. But many organizations are publishing authoritative layers online. In this example, we'll grab the layer directly from the publisher via an [ArcGIS Open Data Hub](https://ucanr-recs-ucanr.opendata.arcgis.com/).

First, we load the libraries we'll need:

```{r setup, message = FALSE}
library(sf)
library(dplyr)
library(tmap)
library(units)
```

\

First we find the URL to the Feature Service. This make take a little exploring, but keep clicking on links that say "Full Details" and "Data Source" until you see a URL that looks like:

<div style="margin-left:2em;"><https://geodata.ucanr.edu/arcgis/rest/services/REC_Data/FeatureServer/0></div>

\

Now we can import it using the [R-ArcGIS](https://www.esri.com/en-us/arcgis/products/r-arcgis-bridge/overview) bridge. Assuming we have an active license to ArcGIS and have installed the package (via ArcGIS Pro), we can load it with:

```{r load_arcgisbinding, cache=TRUE}
library(arcgisbinding)

## verify the license
arc.check_product()
```

\

Now we're ready to import the data:

```{r arc_open, cache = TRUE}
recs_bnd_url <- "https://geodata.ucanr.edu/arcgis/rest/services/REC_Data/FeatureServer/0"

## Create a connection object
recs_bnd_con <- arc.open(path = recs_bnd_url)

## Display info (metadata)
recs_bnd_con
```

\

Import the features and convert them to sf:

```{r arc_select, cache=TRUE}
recs_bnd_sf <- arc.select(recs_bnd_con) %>% arc.data2sf()
recs_bnd_sf
```

\

Inspect the results

```{r tmap_view}
tmap_mode("view")
tm_shape(recs_bnd_sf) + tm_polygons(col = "yellow")
```

\

## Define Our Data Cleaning Needs

Defining the tasks for data prep is typically an iterative process as you build your web map. In this example, our to-do list of operations include:

:::{style="margin-left:1em;"}

1. Keep the `objectid`, `name`, `brochure` fields. Dump everything else.

2. Dissolve (aka union) polygons based on the REC name. As it is now, there are two polygons for 'Lindcove', but we want them to be treated as a single feature in the webmap.

3. Compute the area of each REC to the nearest acre.

4. Standardize the values in the 'name' column (as it is now one of them is abbreviated while the others are spelled out).

5. Add a column for abbreviated names.

6. Create a point version of the layer.

:::

\

We'll go through these operations one-by-one to illustrate the code, but see [below](#compact) for a compact form of the code.

\

### 1\) Keep select fields and dump everything else

To keep only certain columns, use `select()`. 

Note below we won't keep the existing `objectid` column, because once we dissolve boundaries (coming up) there will be one less polygon and we'll need to recreate `objectid` anyway.

Note also we don't have to include the `geom` column (which has the geometry) in `select()`. The geometry column is always included when you use `select()` on a simple feature data frame. 

```{r select_flds}
recs_bnd_sf_2 <- recs_bnd_sf %>% select(name, brochure_l)

recs_bnd_sf_2
```
\

### 2a\) Dissolve or union polygons based on the REC name

You can dissolve / union polygons based on an attribute field by using `group_by()` followed by `summarise()`. Note to keep columns from the attribute table, we have to include them in `summarise()` and tell it how to aggregate values. For discrete fields that are all the same (like names and labels), we can use `first()`.

```{r dissolve}
recs_bnd_sf_3 <- recs_bnd_sf_2 %>% 
  group_by(name) %>% 
  summarise(name = first(name), brochure = first(brochure_l))

recs_bnd_sf_3
```

\

After this step, you can now see that there is only one feature for 'Lindcove'

\

### 2b\) Regenerate `objectid`

We can regenerate a unique id field using `mutate()` with `row_number()`.

```{r objectid}
recs_bnd_sf_4 <- recs_bnd_sf_3 %>% 
  mutate(objectid = row_number())

recs_bnd_sf_4
```

\

### 3\) Compute the area of each REC rounded to the nearest acre

Although the original layer had a field for area in acres, you have to recompute area after any operation that alters geometries (such as dissolve). `st_area()` returns the area of each polygon as a units object (i.e., numeric values with units attached, which it determines from the CRS info). This makes it easy to convert them to our preferred units using `set_units()`: 

```{r compute_acres}
library(units)
recs_bnd_sf_5 <- recs_bnd_sf_4 %>% 
  mutate(acres = st_area(geom) %>% set_units(acres) %>% round(1) %>% as.numeric()) 

recs_bnd_sf_5
```

\

### 4\) Fix values in the 'name' column

You probably noticed that the name of one of the RECs is abbreviated, while all of the others are spelled out. Ideally you would ask the data publisher to fix this. In the meantime we can manually edit this one row this using a vectorized `if_else()` expression:

```{r fix_hrec}
recs_bnd_sf_6 <- recs_bnd_sf_5 %>% 
  mutate(name = if_else(name == "HREC", "Hopland", name))

recs_bnd_sf_6
```

\

### 5\) Add a column for abbreviated name

To add an abbreviated name column programmatically, we could do something similar as above but use `case_when()` to get the correct abbreviation for each REC name. That would be a bit klunky. Instead we'll create a tibble with the abbreviations and join it. This is still klunky, but this approach lends itself to recording the abbreviations in a CSV file, which you can use to bring in other attributes as well.

First we'll create the tibble with the abbreviations:

```{r add_abbrev}
rec_abbrev_tbl <- c("Desert", "DREC",
                    "Hansen", "HAREC",
                    "Hopland", "HREC",
                    "Intermountain", "IREC",
                    "Kearney", "KARE",
                    "Lindcove", "LREC",
                    "Sierra Foothill", "SFREC",
                    "South Coast", "SCREC",
                    "Westside", "WSREC") %>% 
  matrix(ncol = 2, byrow = TRUE) %>% 
  as_tibble(.name_repair = make.names) %>% 
  setNames(c("name", "abbrev"))

rec_abbrev_tbl

```

\

Once we have the lookup table, we can bring in the new column with a simple join:

```{r join_tbls}
recs_bnd_sf_7 <- recs_bnd_sf_6 %>% 
  left_join(rec_abbrev_tbl, by = "name")

recs_bnd_sf_7

```

\

Finally, in preparation for exporting this, let's rearrange the order of the columns:

```{r relocate_cols}
recs_bnd_sf_8 <- recs_bnd_sf_7 %>% 
  relocate(objectid, name, abbrev, acres, brochure, geom)

recs_bnd_sf_8
```

\

We're done with our polygon layer!

\

### 6\) Create a point version of the layer

Finally, let's create a point version of the layer, that we can display on the web map when zoomed out. We'll keep all the attribute fields so we can use the same expressions for labels or popups.

The easiest way to turn polygons into points is to grab the centroid with `st_center()`. For complex geometries we might need to do something more sophisticated, but centroids will work fine for these features:

```{r str_cntr}
recs_ctr_sf <- recs_bnd_sf_8 %>% st_centroid()

recs_ctr_sf
```

\

### Export layers as GeoJSON

Next we'll export our cleaned-up boundary and point objects to GeoJSON:

```{r export_geojson}
st_write(recs_bnd_sf_8, dsn = "recs_bnd_3857.geojson", delete_dsn = TRUE)

st_write(recs_ctr_sf, dsn = "recs_ctr_3857.geojson", delete_dsn = TRUE)
```

\

### Upload to AGOL

We're done cleaning the reference layer. It's now ready to upload to ArcGIS.com. After you upload it, you might want to go into the Feature Service properties and change the **display name** (which becomes the default layer name when you add it to web maps).

If a new version of the authoritative reference layer is released, we simply have to run the commands above to clean it again, and then upload (and replace) the new version. 

\

# Compact Code {#compact}

Above we performed the data processing steps separately to illustrate the process. In a production workflow, you'd probably put these commands in a script. The code snippet below does everything we did above, but much more compactly thanks to the use of pipes.

```{r compact_code, eval = FALSE}
library(sf)
library(dplyr)
library(units)
library(arcgisbinding)

arc.check_product()

epsg_webmerc <- 3857

rec_abbrev_tbl <- c("Desert", "DREC",
                    "Hansen", "HAREC",
                    "Hopland", "HREC",
                    "Intermountain", "IREC",
                    "Kearney", "KARE",
                    "Lindcove", "LREC",
                    "Sierra Foothill", "SFREC",
                    "South Coast", "SCREC",
                    "Westside", "WSREC") %>% 
  matrix(ncol = 2, byrow = TRUE) %>% 
  as_tibble() %>% 
  setNames(c("name", "abbrev"))

recs_bnd_url <- "https://geodata.ucanr.edu/arcgis/rest/services/REC_Data/FeatureServer/0"

recs_bnd_sf <- recs_bnd_url %>% 
  arc.open() %>%                                            ## create connection object
  arc.select() %>%                                          ## import the feature service
  arc.data2sf() %>%                                         ## convert to sf
  st_zm() %>%                                               ## drop z-values (if found)
  st_transform(epsg_webmerc) %>%                            ## convert to web mercator
  select(name, brochure_l) %>%                              ## select fields
  group_by(name) %>%                                        ## group by an attribute
  summarise(name = first(name),                             ## summarize (dissolve boundaries
            brochure = first(brochure_l)) %>%               ## and aggregate attributes)
  mutate(objectid = row_number()) %>%                       ## generate unique values for objectid
  mutate(acres = st_area(geom) %>%                          ## compute area
           set_units(acres) %>%                             ## convert to acres
           round(1) %>%                                     ## round to 1 decimal place
           as.numeric()) %>%                                ## make it numeric
  mutate(name = if_else(name == "HREC",                     ## manually change one value
                        "Hopland", name)) %>% 
  left_join(rec_abbrev_tbl, by = "name") %>%                ## join to the table with abbreviations
  relocate(objectid, name, abbrev, acres, brochure, geom)   ## change the order of columns

## Export polygons
recs_bnd_sf %>% 
  st_write(dsn = "recs_bnd_3857.geojson",                   ## save to disk as geojson
           delete_dsn = TRUE)                               ## overwrite the existing file    

## Export centroids
recs_bnd_sf %>% 
  st_centroid() %>%                                         ## get the centroid of each polygon
  st_write(dsn = "recs_ctr_3857.geojson",                   ## save to disk as geojson
           delete_dsn = TRUE)                               ## overwrite the existing file
```


